{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d98328d",
   "metadata": {},
   "source": [
    "### EmailSpamCorpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5c0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from statistics import mean\n",
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "#import sklearn packages for building classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,StratifiedKFold,cross_val_score,learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad1abe",
   "metadata": {},
   "source": [
    "##### Variables utilized across the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c0cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPath = '/Users/rashmichakravarthy/Desktop/NLP/NLP Project/FinalProjectData/EmailSpamCorpora/corpus'\n",
    "emailDoc = []\n",
    "punctuations = set([\",\", \".\", \"@\", \"#\", \"%\", \"^\", \"&\", \"*\", \"(\", \")\", \"_\", \"-\", \"=\", \"+\", \"{\", \"}\", \"[\", \"]\",\n",
    "             \":\", \";\", \"'\" '\"', \"<\", \">\", \"?\", \"/\"])\n",
    "nltkStopwords = nltk.corpus.stopwords.words('english')\n",
    "moreStopwords = ['could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\"]\n",
    "stopwords = nltkStopwords + moreStopwords\n",
    "bigramMeasures = nltk.collocations.BigramAssocMeasures()\n",
    "predScores_wordVectors = []\n",
    "vocabSize = []\n",
    "wordToidx = {}\n",
    "features = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec836c97",
   "metadata": {},
   "source": [
    "##### Common utitlity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155ffeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions defined to read spam and ham files\n",
    "def process_files_SpamHam(dirPath,limitStr):\n",
    "    spamTexts = []\n",
    "    hamTexts = []\n",
    "    #converting the limit argument to an int from a string\n",
    "    limit = int(limitStr)\n",
    "\n",
    "    #starting lists for spam and ham email texts\n",
    "    os.chdir(dirPath)\n",
    "    #processing files in directory that end in .txt up to a limit, assuming sufficient randomization of emails \n",
    "    for file in os.listdir(\"./spam\"):\n",
    "        if (file.endswith(\".txt\")) and (len(spamTexts) < limit):\n",
    "            #opening file for reading entire file into a string\n",
    "            f = open(\"./spam/\"+file, 'r', encoding=\"latin-1\")\n",
    "            spamTexts.append (f.read())\n",
    "            f.close()\n",
    "    for file in os.listdir(\"./ham\"):\n",
    "        if (file.endswith(\".txt\")) and (len(hamTexts) < limit):\n",
    "            #opening file for reading entire file into a string\n",
    "            f = open(\"./ham/\"+file, 'r', encoding=\"latin-1\")\n",
    "            hamTexts.append (f.read())\n",
    "            f.close()\n",
    "\n",
    "    #printing number of emails read\n",
    "    print (\"Number of spam files:\",len(spamTexts))\n",
    "    print (\"Number of ham files:\",len(hamTexts))\n",
    "\n",
    "    #adding all spam texts\n",
    "    for spam in spamTexts:\n",
    "        tokens = nltk.word_tokenize(spam)\n",
    "        emailDoc.append((tokens, 'spam'))\n",
    "    #adding all regular emails\n",
    "    for ham in hamTexts:\n",
    "        tokens = nltk.word_tokenize(ham)\n",
    "        emailDoc.append((tokens, 'ham'))\n",
    "\n",
    "    #randomizing the list\n",
    "    random.shuffle(emailDoc)\n",
    "\n",
    "    #printing few token lists\n",
    "    for email in emailDoc[:4]:\n",
    "        print (email)\n",
    "        \n",
    "#function defined to removing punctuation\n",
    "def remove_punctuations(emailDoc):\n",
    "    emailDoc_without_punctutations = []\n",
    "    for email in emailDoc:\n",
    "        emailDoc_without_punctutations.append(([token for token in email[0] if token not in punctuations], email[1]))\n",
    "    return emailDoc_without_punctutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a40649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion defined to get word features \n",
    "def get_word_features(emails, num):\n",
    "    allWord_list = []\n",
    "    for email in emails:\n",
    "        allWord_list.extend(email[0])\n",
    "    allWords = nltk.FreqDist(allWord_list)\n",
    "    wordItems = allWords.most_common(num)\n",
    "    wordFeatures = [word for (word, count) in wordItems]\n",
    "    return wordFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7832fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion defined to get TF-IDF score\n",
    "def get_tfidf_scores(emails, num):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    emailCorpus = []\n",
    "    for email in emailDoc:\n",
    "        listToStr = ' '.join([str(element) for element in email[0]])\n",
    "        emailCorpus.append(listToStr)\n",
    "    v = vectorizer.fit_transform(emailCorpus)\n",
    "    feature = vectorizer.get_feature_names()\n",
    "    wordFeatures = feature[:num]\n",
    "    return wordFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c200687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion defined to get unigram bag of words\n",
    "def get_bag_words(email, wordFeatures, Bool=True):\n",
    "    emailWords = email\n",
    "    features = {}\n",
    "    for word in wordFeatures:\n",
    "        if Bool:\n",
    "            features[word] = (word in set(emailWords))\n",
    "        else:\n",
    "            features[word] = emailWords.count(word)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857c885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion defined to get bigram bag of words\n",
    "def get_bigram_bag_words(email, bigramFeatures, Bool=True):\n",
    "    emailBigrams = [\" \".join(bigram) for bigram in ngrams(email, 2)]\n",
    "    features = {}\n",
    "    for bigram in bigramFeatures:\n",
    "        if Bool:\n",
    "            features[bigram] = (bigram in set(emailBigrams))\n",
    "        else:\n",
    "            features[bigram] = emailBigrams.count(bigram)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125149fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion defined to remove stopwords \n",
    "def remove_stopwords(emailDoc, stopwords):\n",
    "    emailDoc_without_stopwords = []\n",
    "    for email in emailDoc:\n",
    "        emailDoc_without_stopwords.append(([token for token in email[0] if token not in stopwords], email[1]))\n",
    "    return emailDoc_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "133c53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_accuracy_evaluation_metrics(foldNums, featureSets):\n",
    "    subsetSize = int(len(featureSets)/foldNums)\n",
    "    print('Each fold size:', subsetSize)\n",
    "    accuracyList = []\n",
    "    gold = []\n",
    "    predicted = []\n",
    "    #iterating over each fold\n",
    "    for f in range(foldNums):\n",
    "        trainingRound = featureSets[:(f*subsetSize)] + featureSets[((f+1)*subsetSize):]\n",
    "        testingRound = featureSets[(f*subsetSize):][:subsetSize]\n",
    "        #training utilizizing trainingRound\n",
    "        classifier = nltk.NaiveBayesClassifier.train(trainingRound)\n",
    "        #evaluating accuracy against testingRound \n",
    "        accuracyRound = nltk.classify.accuracy(classifier, testingRound)\n",
    "        print(\"Accuracy of Fold {}: {}\".format(f, accuracyRound))\n",
    "        accuracyList.append(accuracyRound)\n",
    "        goldList = []\n",
    "        predictedList = []\n",
    "        for (features, label) in testingRound:\n",
    "                gold.append(label)\n",
    "                predicted.append(classifier.classify(features))\n",
    "    #finding the mean accuracy against all rounds\n",
    "    print ('Average accuracy', sum(accuracyList) / foldNums)\n",
    "    #getting a list of labels\n",
    "    labels = list(set(gold))\n",
    "    #depicting each list having values (for each label)\n",
    "    recallList = []\n",
    "    precisionList = []\n",
    "    F1List = []\n",
    "    for label in labels:\n",
    "        #comparing gold and predicted lists for each label while computing their values\n",
    "        TP = TN = FP = FN = 0\n",
    "        for i, value in enumerate(gold):\n",
    "            if value == label and predicted[i] == label:  TP += 1\n",
    "            if value != label and predicted[i] != label:  TN += 1\n",
    "            if value != label and predicted[i] == label:  FP += 1\n",
    "            if value == label and predicted[i] != label:  FN += 1\n",
    "        #utilizing these to compute precision,recall and F1\n",
    "        precision = TP / (TP + FN)\n",
    "        recall = TP / (TP + FP)\n",
    "        recallList.append(recall)\n",
    "        precisionList.append(precision)\n",
    "        F1List.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    #evaluated measures represented in a tabular form (with each row per label)\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # printing measures for each label\n",
    "    for i, label in enumerate(labels):\n",
    "        print(label, '\\t', \"{:8f}\".format(precisionList[i]),\"{:12f}\".format(recallList[i]), \"{:12f}\".format(F1List[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7726ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Tokenizer\n",
    "def custom_tokenizers(email):\n",
    "    tokenizerPattern = r'''(?x)\n",
    "                        [a-z]+(?:['\\-][a-z]+)+\n",
    "                        |[a-z]+\n",
    "                        |\\$\\d+\n",
    "                     '''\n",
    "    return nltk.regexp_tokenize(email.lower().replace(\" ' \", \"'\").replace(\"$ \", \"$\").replace(\" - \", \"-\"), tokenizerPattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32099065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions defined to generate custom tokens from emailDoc\n",
    "def generate_custom_tokens(emailDoc):\n",
    "    newEmail_custom_tokens = []\n",
    "    for email in emailDoc:\n",
    "        listToStr = ' '.join([str(element) for element in email[0]])\n",
    "        newEmail_custom_tokens.append((custom_tokenizers(listToStr), email[1]))\n",
    "    return newEmail_custom_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efe2e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_features_to_dataframe(features):\n",
    "    POS_negMap = {\"spam\": 1, \"ham\": 0}\n",
    "    firstPass = True\n",
    "    df = None\n",
    "    for feature in features:\n",
    "        if firstPass:\n",
    "            df = {f: [] for f in feature[0].keys()}\n",
    "            df[\"label\"] = []\n",
    "            firstPass = False\n",
    "        df[\"label\"].append(POS_negMap[feature[1]])\n",
    "        for f, value in feature[0].items():\n",
    "            df[f].append(value)\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4803452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function defined to evaluate utilizing Scikit-learn Model \n",
    "def get_evaluation_metrics_sklearn(reference, hypothesis):\n",
    "    labels = set(reference)\n",
    "    precisionList = []\n",
    "    recallList = []\n",
    "    F1List = []\n",
    "    correct = 0\n",
    "    for label in labels:\n",
    "        TP = TN = FP = FN = 0\n",
    "        for index, value in enumerate(reference):\n",
    "            if value == label and hypothesis[index] == label:\n",
    "                TP += 1\n",
    "                correct == 1\n",
    "            elif value == label and hypothesis[index] != label: FN += 1\n",
    "            elif value != label and hypothesis[index] == label: FP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "                correct += 1\n",
    "        precision = TP / (TP + FN)\n",
    "        recall = TP / (TP + FP)\n",
    "        recallList.append(recall)\n",
    "        precisionList.append(precision)\n",
    "        F1List.append(2 * (precision * recall ) / (precision + recall))\n",
    "    print(\"Accuracy: {}\".format(round(float(correct) / float(len(reference)), 3)))\n",
    "    print(\"\\n\\tPrecision\\tRecall\\t\\tF1\")\n",
    "    for index, label in enumerate(labels):\n",
    "        print(\"{}\\t{}\\t\\t{}\\t\\t{}\".format(label, round(precisionList[index], 3),\n",
    "                                          round(recallList[index], 3), round(F1List[index], 3)))\n",
    "    print(confusion_matrix(reference, hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebfdc2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    wordVector = np.zeros(vocabSize)\n",
    "    for word in text.split(\" \"):\n",
    "        if wordToidx.get(word) is None:\n",
    "            continue\n",
    "        else:\n",
    "            wordVector[wordToidx.get(word)] += 1\n",
    "    return np.array(wordVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a374bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the data into the models\n",
    "def train(clf, features, targets):\n",
    "    clf.fit(features, targets)\n",
    "\n",
    "def predict(clf, features):\n",
    "    return (clf.predict(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194e208",
   "metadata": {},
   "source": [
    "### STEP - 1\n",
    "##### Process spam/ham files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "461b5aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam files: 1500\n",
      "Number of ham files: 3672\n",
      "(['Subject', ':', 'feeling', 'down', 'about', 'the', 'slze', 'of', 'your', 'johnson', '.', '.', '.', 'rtxyj', 'nxfgr', 'ktrqr', 'abtyw', 'ifpyc', 'edvve', 'smrzz', 'ejwah', 'mgjdq', 'gfsae', 'gnydw', 'mexzx', 'vsdbr', 'lubbp', 'bvdkf', 'otdmbipdtc', 'viukj', 'dnyuv', 'bwekh', 'ctqpm', 'qywdu', 'ywipb', 'stcuy', 'fbnzx', 'slcxz', 'exanh', 'cpxqw', 'rpjiw', 'hbqcu', 'pifce', 'qypyl', 'hntql', 'uignp', 'fpsus', 'wrgcr', 'ymkqh', 'nkzzv', 'wmkmp', 'eoqlt', 'lthje', 'jttsb', 'uhmrq', 'sjkct', 'pqhop', 'gsnoq', 'otvhj', 'ujcrh', 'iagpn', 'baqhr', 'ajdhs', 'ntznk', 'uuzqc', 'kkesa', 'eocwz', 'vaous'], 'spam')\n",
      "(['Subject', ':', '?', '?', '?', '?', '?', '?', '?', '?', 'erp', '!', '?', '?', '?', '?', '?', '?', '?', '?', '½', '?', '?', '?', '?', '?', '?', '?', '?', '?', 'erp', '?', '?', '·', '?', '?', '?', '£', '½', '£', '?', 'erp', '+', '²', '?', '?', '?', '+', '°', '?', '?', '«', '?', '?', '?', '?', '»', '?', '?', '½', '?', '¨', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '²', '?', '·', '½', '°', '?', '£', 'µ', '16', '?', '?', '?', '¦', '?', '?', '?', '£', '?', '?', ',', '?', '?', '²', '½', '?', 'µ', '?', '?', '?', '?', '?', '?', '?', '©', ',', '?', '\\xad', '?', '?', '?', '?', '»', '?', ',', '?', '»', '?', '?', '·', '?', '?', '?', '£', 'µ', '?', '±', '?', '$', '/', '?', '?', '?', '?', '²', '?', '/', '?', '©', '?', '¦', '?', '?', '/', '?', '?', '»', '§', '?', '?', '/', '?', '?', '?', '?', '£', '?', '?', '?', '?', '?', '/', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '½', '?', '¨', '?', '\\xad', '?', '?', '/', '²', '?', '?', '?', '/', '½', '»', '?', '?', '!', '!', '?', '?', '±', '?', '?', '?', '?', '?', '?', '»', '·', '?', '?', '?', '?', '©', '·', '½', '°', '?', '£', '¬', '?', '?', '?', '?', '?', '»', '·', '?', 'erp', '³', '?', '?', '¦', '?', 'r', '?', '·', '±', '¨', '?', '?', '!', '!', 'www', '.', 'mms', '800', '.', 'com', '010', '-', '8532', '-', '5056', '?', '?', '?', '?', '?', '?', '?', '?', '?', '°', '·', '?', '?', '?', '?', '¦', '?', '?', '?', '\\xad', '?', '?', '£', 'µ'], 'spam')\n",
      "(['Subject', ':', 'miracle', 'protein', 'for', 'immune', 'system', \"'\", 'the', 'antidote', \"'\", 'kills', 'all', 'known', 'deadly', 'viruses', 'bacteria', 'in', 'the', 'body', 'that', 'keep', 'diseases', ',', 'namely', ':', 'influenza', ',', 'sars', ',', 'cancer', ',', 'hiv', 'etc', '.', 'a', 'disease', 'must', 'be', 'made', 'dormant', 'to', 'stop', 'infection', '.', \"'\", 'the', 'antidote', \"'\", 'is', 'the', 'answer', '.', 'www', '.', 'alwond', '.', 'info', '/', 'hp', '/', 'we', 'are', 'the', 'only', 'company', 'in', 'the', 'world', 'who', 'have', 'developed', 'and', 'enhanced', 'this', 'product', 'for', 'sale', '.', 'check', 'here', 'for', 'more', 'information', 'www', '.', 'alwond', '.', 'info', '/', 'hp', '/', 'not', 'interested', '?', 'http', ':', '/', '/', 'get', '-', 'it', '-', 'online', '.', 'info', '/', 'soft', '/', 'chair', '.', 'php', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'blemish', 'administer', 'gerhard', 'wheatstone', 'secretive', 'colorate', 'featherweight', 'survey', 'vice', 'loathe', 'virtuoso', 'verbatim', 'epigrammatic', 'eucre', 'farfetched', 'bookplate', 'afforest', 'lily', 'bloomington', 'maroon', 'cloud', 'turnkey', 'cloakroom', 'condemnatory', 'oblivious', 'profound', 'curious', 'declivity', 'implant', 'aid', 'complementation', 'cherubim', 'serpent', 'gleam', 'scud', 'casket', 'scarborough', 'testes', 'actress', 'mawkish', 'beard', 'mace', 'shutoff', 'wooster', 'score', 'minnesota', 'sovereignty', 'efface', 'sixteenth', 'commendatory', 'digestive', 'aliquot', 'backplane', 'bluestocking', 'blumenthal', 'accent', 'desolater', 'freed', 'problematic', 'dutch', 'stab', 'depose', 'trigonometry', 'quetzal', 'accra', 'bel', 'psychophysic', 'surplus', 'ceylon', 'fleawort', 'autocollimate', 'bloom', 'arraign', 'slid', 'inextinguishable', 'prexy', 'gel', 'pivotal', 'olin', 'faraday', 'editorial', 'ir', 'davidson', 'mckee', 'placater', 'damon', 'babysat', 'megalomania', 'dalzell', 'nightgown', 'moe', 'surrender', 'maitre', 'bantam', 'sturgeon', 'tetragonal', 'clearheaded', 'assassinate', 'insufficient', 'stiff', 'repugnant', 'bradshaw', 'parliament', 'july', 'navy', 'threesome', 'you', \"'\", 'd', 'revive', 'china', 'cb', 'retribution', 'elastic', 'constellate', 'kaleidescope', 'sleight', 'adverb', 'chaos', 'thetis', 'discipline', 'jill', 'counterpoint', 'ditty', 'horseflesh', 'obscure', 'bartender', 'close', 'curry', 'posse', 'nsf', 'ave', 'effete', 'leave', 'disdainful', 'sonoma', 'belgrade', 'men', 'headwater', 'medea', 'vivian', 'thirty', 'comic', 'bridgework', 'column', 'chamomile', 'conscription', 'arrack', 'denumerable', 'cognate', 'bedside', 'constipate', 'maureen', 'tenant', 'tremulous', 'belladonna', 'carpentry', 'lithology', 'hog', 'demand', 'transferred', 'playa', 'parley', 'fell', 'hackle', 'osmium', 'delaware', 'bust', 'literal', 'courteous', 'linen', 'bedimmed', 'collagen', 'pulley', 'taurus', 'affect', 'unbidden', 'clayton', 'croix', 'agenda', 'earthshaking', 'gradual', 'sidearm', 'crystal', 'terpsichorean', 'gamut', 'trifluouride', 'midterm', 'practical', 'domesticate', 'bran', 'journalese', 'ovate', 'kovacs', 'pivotal', 'trail', 'camilla', 'armoire', 'attune', 'anatomy', 'ethic', 'emerson', 'sing', 'asbestos', 'premonitory', 'toilet', 'gary', 'sunrise', 'bugging', 'broomcorn', 'fraud', 'billet', 'claude', 'immediacy', 'cutworm', 'debugging', 'dopant', 'suave', 'hind', 'handwrite', 'hour', 'landau', 'atypic', 'seismography', 'outermost', 'eldon', 'emerge', 'carton', 'vivace', 'clifton', 'enormous', 'amy', 'reclamation', 'shrimp', 'clatter', 'basalt', 'jablonsky', 'alumina', 'hydrophobic', 'rufus', 'dora', 'gilead', 'argo', 'youth', 'meniscus', 'wood', 'bulk', 'industry', 'alia', 'eugene', 'didactic', 'topmost', 'draftsman', 'halve', 'dally', 'consistent', 'declarative', 'bandit', 'cypriot', 'douglas', 'sown', 'fungus', 'interdict', 'astronomic', 'provoke', 'tailwind', 'endgame', 'washbowl', 'chiropractor', 'rare', 'fateful', 'ellis', 'polymeric', 'cervantes', 'salina', 'badinage', 'bearish', 'pragmatist', 'confirmation', 'wyoming', 'laud', 'enclave', 'nominee', 'equilateral', 'fatal'], 'spam')\n",
      "(['Subject', ':', 'fw', ':', 'ap', 'wire', 'college', 'station', ',', 'texas', '-', '-', '-', '-', '-', 'original', 'message', '-', '-', '-', '-', '-', 'from', ':', 'beard', ',', 'jaime', 'sent', ':', 'tuesday', ',', 'october', '30', ',', '2001', '1', ':', '16', 'pm', 'to', ':', 'glover', ',', 'rusty', 'subject', ':', 'fw', ':', 'ap', 'wire', 'college', 'station', ',', 'texas', '-', '-', '-', '-', '-', 'original', 'message', '-', '-', '-', '-', '-', 'from', ':', 'teague', ',', 'john', 'sent', ':', 'tuesday', ',', 'october', '30', ',', '2001', '1', ':', '13', 'pm', 'to', ':', 'bennett', ',', 'alex', ';', 'newsom', ',', 'boon', ';', 'herrera', ',', 'ignacio', ';', 'pierce', ',', 'brian', ';', 'warfield', ',', 'dan', ';', 'beard', ',', 'jaime', ';', 'thomas', ',', 'ron', '(', 'r', '.', 't', '.', ')', ';', 'vernon', ',', 'billy', ';', 'ryan', 'lewis', '(', 'e', '-', 'mail', ')', ';', 'ross', 'melton', '(', 'e', '-', 'mail', ')', ';', 'grandpa', '(', 'e', '-', 'mail', ')', ';', 'jeff', 'woodall', '(', 'e', '-', 'mail', ')', ';', 'fuzzy', '(', 'e', '-', 'mail', ')', ';', 'fluffy', '(', 'e', '-', 'mail', ')', 'subject', ':', 'fw', ':', 'ap', 'wire', 'college', 'station', ',', 'texas', 'ap', '-', 'college', 'station', 'texas', '-', '-', '-', '-', 'texas', 'a', '&', 'm', 'football', 'practice', 'was', 'delayed', 'on', 'monday', 'for', 'nearly', 'two', 'hours', 'at', 'kyle', 'field', '.', 'one', 'of', 'the', 'players', ',', 'while', 'on', 'his', 'way', 'to', 'the', 'locker', 'room', 'happened', 'to', 'look', 'down', 'and', 'notice', 'a', 'suspicious', 'looking', ',', 'unknown', 'white', 'powdery', 'substance', 'on', 'the', 'practice', 'field', '.', 'head', 'coach', 'r', '.', 'c', '.', 'slocum', 'immediately', 'suspended', 'practice', 'while', 'the', 'fbi', 'was', 'called', 'in', 'to', 'investigate', '.', 'after', 'a', 'complete', 'field', 'analysis', ',', 'the', 'fbi', 'determined', 'that', 'the', 'white', 'substance', 'unknown', 'to', 'the', 'players', 'was', 'the', 'goal', 'line', '.', 'practice', 'was', 'resumed', 'when', 'fbi', 'special', 'agents', 'decided', 'that', 'the', 'team', 'would', 'not', 'be', 'likely', 'to', 'encounter', 'the', 'substance', 'again', '.'], 'ham')\n"
     ]
    }
   ],
   "source": [
    "process_files_SpamHam(datasetPath, 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08619712",
   "metadata": {},
   "source": [
    "### STEP - 2\n",
    "##### Term Frequency - Inverse Document Frequency (TF-IDF) & 1k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "908150c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.6634429400386848\n",
      "Accuracy of Fold 1: 0.6634429400386848\n",
      "Accuracy of Fold 2: 0.6711798839458414\n",
      "Accuracy of Fold 3: 0.7272727272727273\n",
      "Accuracy of Fold 4: 0.7253384912959381\n",
      "Accuracy of Fold 5: 0.7137330754352031\n",
      "Accuracy of Fold 6: 0.6731141199226306\n",
      "Accuracy of Fold 7: 0.6673114119922631\n",
      "Accuracy of Fold 8: 0.7137330754352031\n",
      "Accuracy of Fold 9: 0.6344294003868471\n",
      "Average accuracy 0.6852998065764024\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.628984     0.897047     0.739472\n",
      "spam \t 0.823215     0.475347     0.602686\n"
     ]
    }
   ],
   "source": [
    "wordFeatures = get_tfidf_scores(emailDoc, 1000)\n",
    "bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec50b24a",
   "metadata": {},
   "source": [
    "##### Utilizing built-in NLTK tokenizer and 1k features to get a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "118aa984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9226305609284333\n",
      "Accuracy of Fold 1: 0.9342359767891683\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9458413926499033\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.9361702127659575\n",
      "Accuracy of Fold 6: 0.9187620889748549\n",
      "Accuracy of Fold 7: 0.9245647969052224\n",
      "Accuracy of Fold 8: 0.9206963249516441\n",
      "Accuracy of Fold 9: 0.9284332688588007\n",
      "Average accuracy 0.929980657640232\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.901662     0.999698     0.948152\n",
      "spam \t 0.999333     0.805810     0.892198\n"
     ]
    }
   ],
   "source": [
    "wordFeatures = get_word_features(emailDoc, 1000)\n",
    "bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ccfb56",
   "metadata": {},
   "source": [
    "##### Changing the size of feature set with TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d4859a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 500\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.6692456479690522\n",
      "Accuracy of Fold 1: 0.6518375241779497\n",
      "Accuracy of Fold 2: 0.6634429400386848\n",
      "Accuracy of Fold 3: 0.7001934235976789\n",
      "Accuracy of Fold 4: 0.7137330754352031\n",
      "Accuracy of Fold 5: 0.7156673114119922\n",
      "Accuracy of Fold 6: 0.6808510638297872\n",
      "Accuracy of Fold 7: 0.6479690522243714\n",
      "Accuracy of Fold 8: 0.7021276595744681\n",
      "Accuracy of Fold 9: 0.6382978723404256\n",
      "Average accuracy 0.6783365570599613\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.633342     0.880015     0.736575\n",
      "spam \t 0.788526     0.467563     0.587037\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 1000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.6634429400386848\n",
      "Accuracy of Fold 1: 0.6634429400386848\n",
      "Accuracy of Fold 2: 0.6711798839458414\n",
      "Accuracy of Fold 3: 0.7272727272727273\n",
      "Accuracy of Fold 4: 0.7253384912959381\n",
      "Accuracy of Fold 5: 0.7137330754352031\n",
      "Accuracy of Fold 6: 0.6731141199226306\n",
      "Accuracy of Fold 7: 0.6673114119922631\n",
      "Accuracy of Fold 8: 0.7137330754352031\n",
      "Accuracy of Fold 9: 0.6344294003868471\n",
      "Average accuracy 0.6852998065764024\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.628984     0.897047     0.739472\n",
      "spam \t 0.823215     0.475347     0.602686\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 2500\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.816247582205029\n",
      "Accuracy of Fold 1: 0.7872340425531915\n",
      "Accuracy of Fold 2: 0.8065764023210832\n",
      "Accuracy of Fold 3: 0.8201160541586073\n",
      "Accuracy of Fold 4: 0.8452611218568665\n",
      "Accuracy of Fold 5: 0.7988394584139265\n",
      "Accuracy of Fold 6: 0.7659574468085106\n",
      "Accuracy of Fold 7: 0.8181818181818182\n",
      "Accuracy of Fold 8: 0.8220502901353965\n",
      "Accuracy of Fold 9: 0.7988394584139265\n",
      "Average accuracy 0.8079303675048356\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.752928     0.969825     0.847723\n",
      "spam \t 0.942628     0.609052     0.739984\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 3000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.8104448742746615\n",
      "Accuracy of Fold 1: 0.7911025145067698\n",
      "Accuracy of Fold 2: 0.8104448742746615\n",
      "Accuracy of Fold 3: 0.8143133462282398\n",
      "Accuracy of Fold 4: 0.8491295938104448\n",
      "Accuracy of Fold 5: 0.7949709864603481\n",
      "Accuracy of Fold 6: 0.7620889748549323\n",
      "Accuracy of Fold 7: 0.8201160541586073\n",
      "Accuracy of Fold 8: 0.8181818181818182\n",
      "Accuracy of Fold 9: 0.8007736943907157\n",
      "Average accuracy 0.8071566731141198\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.751294     0.970443     0.846922\n",
      "spam \t 0.943963     0.607818     0.739483\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 5500\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.8781431334622823\n",
      "Accuracy of Fold 1: 0.839458413926499\n",
      "Accuracy of Fold 2: 0.8704061895551257\n",
      "Accuracy of Fold 3: 0.8704061895551257\n",
      "Accuracy of Fold 4: 0.9032882011605415\n",
      "Accuracy of Fold 5: 0.8626692456479691\n",
      "Accuracy of Fold 6: 0.8491295938104448\n",
      "Accuracy of Fold 7: 0.8665377176015474\n",
      "Accuracy of Fold 8: 0.8762088974854932\n",
      "Accuracy of Fold 9: 0.8433268858800773\n",
      "Average accuracy 0.8659574468085106\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.827840     0.980323     0.897652\n",
      "spam \t 0.959306     0.694686     0.805828\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 7000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.8897485493230174\n",
      "Accuracy of Fold 1: 0.8878143133462283\n",
      "Accuracy of Fold 2: 0.8916827852998066\n",
      "Accuracy of Fold 3: 0.8936170212765957\n",
      "Accuracy of Fold 4: 0.8800773694390716\n",
      "Accuracy of Fold 5: 0.8878143133462283\n",
      "Accuracy of Fold 6: 0.8897485493230174\n",
      "Accuracy of Fold 7: 0.8994197292069632\n",
      "Accuracy of Fold 8: 0.8897485493230174\n",
      "Accuracy of Fold 9: 0.8684719535783365\n",
      "Average accuracy 0.8878143133462283\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.936530     0.908322     0.922210\n",
      "spam \t 0.768512     0.831769     0.798890\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 10000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.8916827852998066\n",
      "Accuracy of Fold 1: 0.8936170212765957\n",
      "Accuracy of Fold 2: 0.9110251450676983\n",
      "Accuracy of Fold 3: 0.9090909090909091\n",
      "Accuracy of Fold 4: 0.8955512572533849\n",
      "Accuracy of Fold 5: 0.8936170212765957\n",
      "Accuracy of Fold 6: 0.8839458413926499\n",
      "Accuracy of Fold 7: 0.9052224371373307\n",
      "Accuracy of Fold 8: 0.8955512572533849\n",
      "Accuracy of Fold 9: 0.8897485493230174\n",
      "Average accuracy 0.8969052224371372\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.976028     0.889523     0.930770\n",
      "spam \t 0.703135     0.922942     0.798183\n"
     ]
    }
   ],
   "source": [
    "for size in [500, 1000, 2500, 3000, 5500, 7000, 10000]:\n",
    "    print(\"\\n---------------------------------------------------------------\")\n",
    "    print(\"\\n\\tResults for varying size of vocabulary {}\".format(size))\n",
    "    print(\"\\n---------------------------------------------------------------\")\n",
    "    wordFeatures = get_tfidf_scores(emailDoc, size)\n",
    "    bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in emailDoc]\n",
    "    cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296bfbad",
   "metadata": {},
   "source": [
    "##### Changing the size of feature set in baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "581c3f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 500\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9187620889748549\n",
      "Accuracy of Fold 1: 0.9148936170212766\n",
      "Accuracy of Fold 2: 0.9110251450676983\n",
      "Accuracy of Fold 3: 0.9264990328820116\n",
      "Accuracy of Fold 4: 0.9361702127659575\n",
      "Accuracy of Fold 5: 0.9168278529980658\n",
      "Accuracy of Fold 6: 0.8955512572533849\n",
      "Accuracy of Fold 7: 0.9052224371373307\n",
      "Accuracy of Fold 8: 0.90715667311412\n",
      "Accuracy of Fold 9: 0.9129593810444874\n",
      "Average accuracy 0.9145067698259188\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.882048     0.997228     0.936109\n",
      "spam \t 0.993996     0.774831     0.870836\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 1000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9226305609284333\n",
      "Accuracy of Fold 1: 0.9342359767891683\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9458413926499033\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.9361702127659575\n",
      "Accuracy of Fold 6: 0.9187620889748549\n",
      "Accuracy of Fold 7: 0.9245647969052224\n",
      "Accuracy of Fold 8: 0.9206963249516441\n",
      "Accuracy of Fold 9: 0.9284332688588007\n",
      "Average accuracy 0.929980657640232\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.901662     0.999698     0.948152\n",
      "spam \t 0.999333     0.805810     0.892198\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 2500\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9613152804642167\n",
      "Accuracy of Fold 1: 0.9671179883945842\n",
      "Accuracy of Fold 2: 0.9400386847195358\n",
      "Accuracy of Fold 3: 0.9690522243713733\n",
      "Accuracy of Fold 4: 0.9632495164410058\n",
      "Accuracy of Fold 5: 0.9593810444874274\n",
      "Accuracy of Fold 6: 0.9613152804642167\n",
      "Accuracy of Fold 7: 0.9400386847195358\n",
      "Accuracy of Fold 8: 0.9439071566731141\n",
      "Accuracy of Fold 9: 0.9477756286266924\n",
      "Average accuracy 0.9553191489361703\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.938436     0.998551     0.967561\n",
      "spam \t 0.996664     0.868605     0.928239\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 3000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9632495164410058\n",
      "Accuracy of Fold 1: 0.9671179883945842\n",
      "Accuracy of Fold 2: 0.9439071566731141\n",
      "Accuracy of Fold 3: 0.9690522243713733\n",
      "Accuracy of Fold 4: 0.9632495164410058\n",
      "Accuracy of Fold 5: 0.965183752417795\n",
      "Accuracy of Fold 6: 0.9613152804642167\n",
      "Accuracy of Fold 7: 0.941972920696325\n",
      "Accuracy of Fold 8: 0.9439071566731141\n",
      "Accuracy of Fold 9: 0.9497098646034816\n",
      "Average accuracy 0.9568665377176014\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.941433     0.997691     0.968746\n",
      "spam \t 0.994663     0.873974     0.930421\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 5500\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.965183752417795\n",
      "Accuracy of Fold 1: 0.9632495164410058\n",
      "Accuracy of Fold 2: 0.9458413926499033\n",
      "Accuracy of Fold 3: 0.9709864603481625\n",
      "Accuracy of Fold 4: 0.9477756286266924\n",
      "Accuracy of Fold 5: 0.9516441005802708\n",
      "Accuracy of Fold 6: 0.9516441005802708\n",
      "Accuracy of Fold 7: 0.9555125725338491\n",
      "Accuracy of Fold 8: 0.9535783365570599\n",
      "Accuracy of Fold 9: 0.9516441005802708\n",
      "Average accuracy 0.9557059961315282\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.952601     0.984516     0.968296\n",
      "spam \t 0.963309     0.892460     0.926532\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 7000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9613152804642167\n",
      "Accuracy of Fold 1: 0.9477756286266924\n",
      "Accuracy of Fold 2: 0.9516441005802708\n",
      "Accuracy of Fold 3: 0.965183752417795\n",
      "Accuracy of Fold 4: 0.9477756286266924\n",
      "Accuracy of Fold 5: 0.9497098646034816\n",
      "Accuracy of Fold 6: 0.9516441005802708\n",
      "Accuracy of Fold 7: 0.9555125725338491\n",
      "Accuracy of Fold 8: 0.9516441005802708\n",
      "Accuracy of Fold 9: 0.9361702127659575\n",
      "Average accuracy 0.9518375241779499\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.955053     0.976602     0.965707\n",
      "spam \t 0.943963     0.895570     0.919130\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 10000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9555125725338491\n",
      "Accuracy of Fold 1: 0.9439071566731141\n",
      "Accuracy of Fold 2: 0.9477756286266924\n",
      "Accuracy of Fold 3: 0.9555125725338491\n",
      "Accuracy of Fold 4: 0.9226305609284333\n",
      "Accuracy of Fold 5: 0.9303675048355899\n",
      "Accuracy of Fold 6: 0.9381044487427466\n",
      "Accuracy of Fold 7: 0.9593810444874274\n",
      "Accuracy of Fold 8: 0.9458413926499033\n",
      "Accuracy of Fold 9: 0.9303675048355899\n",
      "Average accuracy 0.9429400386847195\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.959956     0.959695     0.959826\n",
      "spam \t 0.901268     0.901869     0.901568\n"
     ]
    }
   ],
   "source": [
    "for size in [500, 1000, 2500, 3000, 5500, 7000, 10000]:\n",
    "    print(\"\\n---------------------------------------------------------------\")\n",
    "    print(\"\\n\\tResults for varying size of vocabulary {}\".format(size))\n",
    "    print(\"\\n---------------------------------------------------------------\")\n",
    "    wordFeatures = get_word_features(emailDoc, size)\n",
    "    bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in emailDoc]\n",
    "    cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdabd1",
   "metadata": {},
   "source": [
    "##### Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab4049a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9323017408123792\n",
      "Accuracy of Fold 1: 0.9400386847195358\n",
      "Accuracy of Fold 2: 0.9264990328820116\n",
      "Accuracy of Fold 3: 0.9458413926499033\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.9381044487427466\n",
      "Accuracy of Fold 6: 0.9323017408123792\n",
      "Accuracy of Fold 7: 0.9226305609284333\n",
      "Accuracy of Fold 8: 0.9245647969052224\n",
      "Accuracy of Fold 9: 0.9284332688588007\n",
      "Average accuracy 0.9336557059961315\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.906837     0.999700     0.951007\n",
      "spam \t 0.999333     0.814130     0.897275\n"
     ]
    }
   ],
   "source": [
    "remove_punctuations_emailDoc = remove_punctuations(emailDoc)\n",
    "wordFeatures = get_word_features(remove_punctuations_emailDoc, 1000)\n",
    "bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in remove_punctuations_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2702985",
   "metadata": {},
   "source": [
    "##### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f05dd202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9284332688588007\n",
      "Accuracy of Fold 1: 0.9381044487427466\n",
      "Accuracy of Fold 2: 0.9245647969052224\n",
      "Accuracy of Fold 3: 0.9516441005802708\n",
      "Accuracy of Fold 4: 0.9477756286266924\n",
      "Accuracy of Fold 5: 0.9342359767891683\n",
      "Accuracy of Fold 6: 0.9148936170212766\n",
      "Accuracy of Fold 7: 0.9245647969052224\n",
      "Accuracy of Fold 8: 0.9342359767891683\n",
      "Accuracy of Fold 9: 0.9284332688588007\n",
      "Average accuracy 0.9326885880077368\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.906020     0.999099     0.950286\n",
      "spam \t 0.997999     0.812602     0.895808\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords_emailDoc = remove_stopwords(emailDoc, stopwords)\n",
    "wordFeatures = get_word_features(remove_stopwords_emailDoc, 1000)\n",
    "bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in remove_stopwords_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a062c2",
   "metadata": {},
   "source": [
    "##### Removing punctuations & stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a47122e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9361702127659575\n",
      "Accuracy of Fold 1: 0.9361702127659575\n",
      "Accuracy of Fold 2: 0.9245647969052224\n",
      "Accuracy of Fold 3: 0.9555125725338491\n",
      "Accuracy of Fold 4: 0.9516441005802708\n",
      "Accuracy of Fold 5: 0.9400386847195358\n",
      "Accuracy of Fold 6: 0.9226305609284333\n",
      "Accuracy of Fold 7: 0.9323017408123792\n",
      "Accuracy of Fold 8: 0.9226305609284333\n",
      "Accuracy of Fold 9: 0.9284332688588007\n",
      "Average accuracy 0.9350096711798839\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.909561     0.998803     0.952096\n",
      "spam \t 0.997332     0.818281     0.898978\n"
     ]
    }
   ],
   "source": [
    "remove_punctuations_emailDoc = remove_punctuations(emailDoc)\n",
    "remove_stopwords_emailDoc = remove_stopwords(remove_punctuations_emailDoc, stopwords)\n",
    "wordFeatures = get_word_features(remove_stopwords_emailDoc, 1000)\n",
    "bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in remove_stopwords_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b33f4",
   "metadata": {},
   "source": [
    "##### Custom tokenizer on emailDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd3c5943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9342359767891683\n",
      "Accuracy of Fold 1: 0.941972920696325\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9593810444874274\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9303675048355899\n",
      "Accuracy of Fold 8: 0.9226305609284333\n",
      "Accuracy of Fold 9: 0.9342359767891683\n",
      "Average accuracy 0.936750483558994\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.912558     0.998212     0.953465\n",
      "spam \t 0.995997     0.823043     0.901298\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer_emailDoc = generate_custom_tokens(emailDoc)\n",
    "wordFeatures = get_word_features(custom_tokenizer_emailDoc, 1000)\n",
    "bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in custom_tokenizer_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d2ee8",
   "metadata": {},
   "source": [
    "##### Custom tokenizer on emailDoc while having punctuations removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60e5a36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9342359767891683\n",
      "Accuracy of Fold 1: 0.941972920696325\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9593810444874274\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9303675048355899\n",
      "Accuracy of Fold 8: 0.9226305609284333\n",
      "Accuracy of Fold 9: 0.9342359767891683\n",
      "Average accuracy 0.936750483558994\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.912558     0.998212     0.953465\n",
      "spam \t 0.995997     0.823043     0.901298\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer_emailDoc = generate_custom_tokens(emailDoc)\n",
    "remove_punctuations_emailDoc = remove_punctuations(custom_tokenizer_emailDoc)\n",
    "wordFeatures = get_word_features(custom_tokenizer_emailDoc, 1000)\n",
    "bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in remove_punctuations_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ddd27",
   "metadata": {},
   "source": [
    "##### Custom tokenizer on emailDoc while having stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8ba86de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9361702127659575\n",
      "Accuracy of Fold 1: 0.9439071566731141\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9555125725338491\n",
      "Accuracy of Fold 4: 0.9439071566731141\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9323017408123792\n",
      "Accuracy of Fold 8: 0.9245647969052224\n",
      "Accuracy of Fold 9: 0.9323017408123792\n",
      "Average accuracy 0.9367504835589943\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.913648     0.997027     0.953518\n",
      "spam \t 0.993329     0.824474     0.901059\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer_emailDoc = generate_custom_tokens(emailDoc)\n",
    "remove_stopwords_emailDoc = remove_stopwords(custom_tokenizer_emailDoc, stopwords)\n",
    "wordFeatures = get_word_features(custom_tokenizer_emailDoc, 1000)\n",
    "bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in remove_stopwords_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916d637",
   "metadata": {},
   "source": [
    "##### Custom tokenizer on emailDoc while having punctuations & stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba8523b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9361702127659575\n",
      "Accuracy of Fold 1: 0.9439071566731141\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9555125725338491\n",
      "Accuracy of Fold 4: 0.9439071566731141\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9323017408123792\n",
      "Accuracy of Fold 8: 0.9245647969052224\n",
      "Accuracy of Fold 9: 0.9323017408123792\n",
      "Average accuracy 0.9367504835589943\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.913648     0.997027     0.953518\n",
      "spam \t 0.993329     0.824474     0.901059\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer_emailDoc = generate_custom_tokens(emailDoc)\n",
    "remove_punctuations_emailDoc = remove_punctuations(custom_tokenizer_emailDoc)\n",
    "remove_stopwords_emailDoc = remove_stopwords(remove_punctuations_emailDoc, stopwords)\n",
    "word_features = get_word_features(remove_stopwords_emailDoc, 1000)\n",
    "bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in remove_stopwords_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424842b",
   "metadata": {},
   "source": [
    "##### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfd47006",
   "metadata": {},
   "outputs": [],
   "source": [
    "emailTokens = [token for email in emailDoc for token in email[0]]\n",
    "emailBigram_finder = BigramCollocationFinder.from_words(emailTokens)\n",
    "email_bigrams_freq = emailBigram_finder.score_ngrams(bigramMeasures.raw_freq)\n",
    "email_bigrams_pmi = emailBigram_finder.score_ngrams(bigramMeasures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17c88461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tFrequency\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.8646034816247582\n",
      "Accuracy of Fold 1: 0.8239845261121856\n",
      "Accuracy of Fold 2: 0.8529980657640233\n",
      "Accuracy of Fold 3: 0.8762088974854932\n",
      "Accuracy of Fold 4: 0.8646034816247582\n",
      "Accuracy of Fold 5: 0.8646034816247582\n",
      "Accuracy of Fold 6: 0.8375241779497099\n",
      "Accuracy of Fold 7: 0.8491295938104448\n",
      "Accuracy of Fold 8: 0.8568665377176016\n",
      "Accuracy of Fold 9: 0.839458413926499\n",
      "Average accuracy 0.8529980657640233\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.795696     0.996588     0.884883\n",
      "spam \t 0.993329     0.665029     0.796683\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tPMI\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.7388781431334622\n",
      "Accuracy of Fold 1: 0.7524177949709865\n",
      "Accuracy of Fold 2: 0.758220502901354\n",
      "Accuracy of Fold 3: 0.7311411992263056\n",
      "Accuracy of Fold 4: 0.7137330754352031\n",
      "Accuracy of Fold 5: 0.7427466150870407\n",
      "Accuracy of Fold 6: 0.7253384912959381\n",
      "Accuracy of Fold 7: 0.7001934235976789\n",
      "Accuracy of Fold 8: 0.7311411992263056\n",
      "Accuracy of Fold 9: 0.7117988394584139\n",
      "Average accuracy 0.7305609284332688\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.994552     0.726712     0.839793\n",
      "spam \t 0.084056     0.863014     0.153191\n"
     ]
    }
   ],
   "source": [
    "email_bigrams_freqList = [\" \".join(bigram[0]) for bigram in email_bigrams_freq[:1000]]\n",
    "email_bigrams_pmiList = [\" \".join(bigram[0]) for bigram in email_bigrams_pmi[:1000]]\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tFrequency\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_freqList), label) for (email, label) in emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tPMI\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_pmiList), label) for (email, label) in emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a8e87",
   "metadata": {},
   "source": [
    "##### Bigrams having punctuations removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb67b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuations_emailDoc = remove_punctuations(emailDoc)\n",
    "emailTokens = [token for email in remove_punctuations_emailDoc for token in email[0]]\n",
    "emailBigram_finder = BigramCollocationFinder.from_words(emailTokens)\n",
    "email_bigrams_freq = emailBigram_finder.score_ngrams(bigramMeasures.raw_freq)\n",
    "email_bigrams_pmi = emailBigram_finder.score_ngrams(bigramMeasures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80d5ab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tFrequency\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.8646034816247582\n",
      "Accuracy of Fold 1: 0.851063829787234\n",
      "Accuracy of Fold 2: 0.8355899419729207\n",
      "Accuracy of Fold 3: 0.851063829787234\n",
      "Accuracy of Fold 4: 0.8897485493230174\n",
      "Accuracy of Fold 5: 0.8646034816247582\n",
      "Accuracy of Fold 6: 0.8491295938104448\n",
      "Accuracy of Fold 7: 0.8336557059961315\n",
      "Accuracy of Fold 8: 0.8471953578336557\n",
      "Accuracy of Fold 9: 0.8529980657640233\n",
      "Average accuracy 0.8539651837524177\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.797603     0.995918     0.885796\n",
      "spam \t 0.991995     0.666816     0.797533\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tPMI\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.7350096711798839\n",
      "Accuracy of Fold 1: 0.7427466150870407\n",
      "Accuracy of Fold 2: 0.7543520309477756\n",
      "Accuracy of Fold 3: 0.7272727272727273\n",
      "Accuracy of Fold 4: 0.7079303675048356\n",
      "Accuracy of Fold 5: 0.7388781431334622\n",
      "Accuracy of Fold 6: 0.7214700193423598\n",
      "Accuracy of Fold 7: 0.6924564796905223\n",
      "Accuracy of Fold 8: 0.7272727272727273\n",
      "Accuracy of Fold 9: 0.7079303675048356\n",
      "Average accuracy 0.725531914893617\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.990738     0.724213     0.836765\n",
      "spam \t 0.076051     0.770270     0.138434\n"
     ]
    }
   ],
   "source": [
    "email_bigrams_freqList = [\" \".join(bigram[0]) for bigram in email_bigrams_freq[:1000]]\n",
    "email_bigrams_pmiList = [\" \".join(bigram[0]) for bigram in email_bigrams_pmi[:1000]]\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tFrequency\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_freqList), label) for (email, label) in remove_punctuations_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tPMI\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_pmiList), label) for (email, label) in remove_punctuations_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85ad08",
   "metadata": {},
   "source": [
    "##### Bigrams having stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a13c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords_emailDoc = remove_stopwords(emailDoc, stopwords)\n",
    "emailTokens = [token for email in remove_stopwords_emailDoc for token in email[0]]\n",
    "emailBigram_finder = BigramCollocationFinder.from_words(emailTokens)\n",
    "email_bigrams_freq = emailBigram_finder.score_ngrams(bigramMeasures.raw_freq)\n",
    "email_bigrams_pmi = emailBigram_finder.score_ngrams(bigramMeasures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe9c9c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tFrequency\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.8588007736943907\n",
      "Accuracy of Fold 1: 0.816247582205029\n",
      "Accuracy of Fold 2: 0.8297872340425532\n",
      "Accuracy of Fold 3: 0.839458413926499\n",
      "Accuracy of Fold 4: 0.8452611218568665\n",
      "Accuracy of Fold 5: 0.8471953578336557\n",
      "Accuracy of Fold 6: 0.8181818181818182\n",
      "Accuracy of Fold 7: 0.8201160541586073\n",
      "Accuracy of Fold 8: 0.8375241779497099\n",
      "Accuracy of Fold 9: 0.8123791102514507\n",
      "Average accuracy 0.832495164410058\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.766276     0.997164     0.866605\n",
      "spam \t 0.994663     0.634738     0.774948\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tPMI\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.7330754352030948\n",
      "Accuracy of Fold 1: 0.7446808510638298\n",
      "Accuracy of Fold 2: 0.7601547388781431\n",
      "Accuracy of Fold 3: 0.7292069632495164\n",
      "Accuracy of Fold 4: 0.7137330754352031\n",
      "Accuracy of Fold 5: 0.7408123791102514\n",
      "Accuracy of Fold 6: 0.7272727272727273\n",
      "Accuracy of Fold 7: 0.6982591876208898\n",
      "Accuracy of Fold 8: 0.7292069632495164\n",
      "Accuracy of Fold 9: 0.7117988394584139\n",
      "Average accuracy 0.7288201160541586\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.993462     0.725771     0.838776\n",
      "spam \t 0.080720     0.834483     0.147202\n"
     ]
    }
   ],
   "source": [
    "email_bigrams_freqList = [\" \".join(bigram[0]) for bigram in email_bigrams_freq[:1000]]\n",
    "email_bigrams_pmiList = [\" \".join(bigram[0]) for bigram in email_bigrams_pmi[:1000]]\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tFrequency\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_freqList), label) for (email, label) in remove_stopwords_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tPMI\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_pmiList), label) for (email, label) in remove_stopwords_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bab9d",
   "metadata": {},
   "source": [
    "##### Bigrams having punctuations & stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abb3b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuations_emailDoc = remove_punctuations(emailDoc)\n",
    "remove_stopwords_emailDoc = remove_stopwords(remove_punctuations_emailDoc, stopwords)\n",
    "emailTokens = [token for email in remove_stopwords_emailDoc for token in email[0]]\n",
    "emailBigram_finder = BigramCollocationFinder.from_words(emailTokens)\n",
    "email_bigrams_freq = emailBigram_finder.score_ngrams(bigramMeasures.raw_freq)\n",
    "email_bigrams_pmi = emailBigram_finder.score_ngrams(bigramMeasures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34f47e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tFrequency\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.8588007736943907\n",
      "Accuracy of Fold 1: 0.8123791102514507\n",
      "Accuracy of Fold 2: 0.8104448742746615\n",
      "Accuracy of Fold 3: 0.8317214700193424\n",
      "Accuracy of Fold 4: 0.8607350096711799\n",
      "Accuracy of Fold 5: 0.8433268858800773\n",
      "Accuracy of Fold 6: 0.816247582205029\n",
      "Accuracy of Fold 7: 0.8143133462282398\n",
      "Accuracy of Fold 8: 0.8413926499032882\n",
      "Accuracy of Fold 9: 0.7988394584139265\n",
      "Average accuracy 0.8288201160541586\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.759466     0.999283     0.863024\n",
      "spam \t 0.998666     0.628992     0.771848\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tPMI\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.7330754352030948\n",
      "Accuracy of Fold 1: 0.7408123791102514\n",
      "Accuracy of Fold 2: 0.7524177949709865\n",
      "Accuracy of Fold 3: 0.7272727272727273\n",
      "Accuracy of Fold 4: 0.7001934235976789\n",
      "Accuracy of Fold 5: 0.7408123791102514\n",
      "Accuracy of Fold 6: 0.7214700193423598\n",
      "Accuracy of Fold 7: 0.6924564796905223\n",
      "Accuracy of Fold 8: 0.7272727272727273\n",
      "Accuracy of Fold 9: 0.7098646034816247\n",
      "Average accuracy 0.7245647969052224\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.989104     0.724028     0.836058\n",
      "spam \t 0.076718     0.741935     0.139057\n"
     ]
    }
   ],
   "source": [
    "email_bigrams_freqList = [\" \".join(bigram[0]) for bigram in email_bigrams_freq[:1000]]\n",
    "email_bigrams_pmiList = [\" \".join(bigram[0]) for bigram in email_bigrams_pmi[:1000]]\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tFrequency\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_freqList), label) for (email, label) in remove_stopwords_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tPMI\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_pmiList), label) for (email, label) in remove_stopwords_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa46c6",
   "metadata": {},
   "source": [
    "##### Bigrams with custom tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd4fbebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokenizer_emailDoc = generate_custom_tokens(emailDoc)\n",
    "emailTokens = [token for email in custom_tokenizer_emailDoc for token in email[0]]\n",
    "emailBigram_finder = BigramCollocationFinder.from_words(emailTokens)\n",
    "email_bigrams_freq = emailBigram_finder.score_ngrams(bigramMeasures.raw_freq)\n",
    "email_bigrams_pmi = emailBigram_finder.score_ngrams(bigramMeasures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "632e6872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tFrequency\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.8704061895551257\n",
      "Accuracy of Fold 1: 0.8665377176015474\n",
      "Accuracy of Fold 2: 0.8471953578336557\n",
      "Accuracy of Fold 3: 0.8626692456479691\n",
      "Accuracy of Fold 4: 0.8878143133462283\n",
      "Accuracy of Fold 5: 0.8704061895551257\n",
      "Accuracy of Fold 6: 0.8452611218568665\n",
      "Accuracy of Fold 7: 0.8433268858800773\n",
      "Accuracy of Fold 8: 0.8684719535783365\n",
      "Accuracy of Fold 9: 0.8607350096711799\n",
      "Average accuracy 0.8622823984526112\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.810951     0.993990     0.893189\n",
      "spam \t 0.987992     0.680920     0.806206\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tPMI\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.7369439071566731\n",
      "Accuracy of Fold 1: 0.7485493230174082\n",
      "Accuracy of Fold 2: 0.7620889748549323\n",
      "Accuracy of Fold 3: 0.7292069632495164\n",
      "Accuracy of Fold 4: 0.7117988394584139\n",
      "Accuracy of Fold 5: 0.7446808510638298\n",
      "Accuracy of Fold 6: 0.7292069632495164\n",
      "Accuracy of Fold 7: 0.7021276595744681\n",
      "Accuracy of Fold 8: 0.7388781431334622\n",
      "Accuracy of Fold 9: 0.7195357833655706\n",
      "Average accuracy 0.732301740812379\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.996731     0.727291     0.840956\n",
      "spam \t 0.084723     0.913669     0.155067\n"
     ]
    }
   ],
   "source": [
    "email_bigrams_freqList = [\" \".join(bigram[0]) for bigram in email_bigrams_freq[:1000]]\n",
    "email_bigrams_pmiList = [\" \".join(bigram[0]) for bigram in email_bigrams_pmi[:1000]]\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tFrequency\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_freqList), label) for (email, label) in custom_tokenizer_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tPMI\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_pmiList), label) for (email, label) in custom_tokenizer_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946d93a",
   "metadata": {},
   "source": [
    "##### Bigrams with custom tokenizers while having stopwords removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d1997d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokenizer_emailDoc = generate_custom_tokens(emailDoc)\n",
    "remove_stopwords_emailDoc = remove_stopwords(custom_tokenizer_emailDoc, stopwords)\n",
    "emailTokens = [token for email in custom_tokenizer_emailDoc for token in email[0]]\n",
    "emailBigram_finder = BigramCollocationFinder.from_words(emailTokens)\n",
    "email_bigrams_freq = emailBigram_finder.score_ngrams(bigramMeasures.raw_freq)\n",
    "email_bigrams_pmi = emailBigram_finder.score_ngrams(bigramMeasures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68f2e818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tFrequency\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.8143133462282398\n",
      "Accuracy of Fold 1: 0.7524177949709865\n",
      "Accuracy of Fold 2: 0.7369439071566731\n",
      "Accuracy of Fold 3: 0.7524177949709865\n",
      "Accuracy of Fold 4: 0.7872340425531915\n",
      "Accuracy of Fold 5: 0.793036750483559\n",
      "Accuracy of Fold 6: 0.7620889748549323\n",
      "Accuracy of Fold 7: 0.7562862669245648\n",
      "Accuracy of Fold 8: 0.7891682785299806\n",
      "Accuracy of Fold 9: 0.7524177949709865\n",
      "Average accuracy 0.76963249516441\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.676382     0.998793     0.806562\n",
      "spam \t 0.997999     0.557377     0.715276\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tPMI\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.7369439071566731\n",
      "Accuracy of Fold 1: 0.7485493230174082\n",
      "Accuracy of Fold 2: 0.7620889748549323\n",
      "Accuracy of Fold 3: 0.7292069632495164\n",
      "Accuracy of Fold 4: 0.7117988394584139\n",
      "Accuracy of Fold 5: 0.7446808510638298\n",
      "Accuracy of Fold 6: 0.7292069632495164\n",
      "Accuracy of Fold 7: 0.7021276595744681\n",
      "Accuracy of Fold 8: 0.7388781431334622\n",
      "Accuracy of Fold 9: 0.7195357833655706\n",
      "Average accuracy 0.732301740812379\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.996731     0.727291     0.840956\n",
      "spam \t 0.084723     0.913669     0.155067\n"
     ]
    }
   ],
   "source": [
    "email_bigrams_freqList = [\" \".join(bigram[0]) for bigram in email_bigrams_freq[:1000]]\n",
    "email_bigrams_pmiList = [\" \".join(bigram[0]) for bigram in email_bigrams_pmi[:1000]]\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tFrequency\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_freqList), label) for (email, label) in remove_stopwords_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "print(\"\\n\\tPMI\".format(1000))\n",
    "print(\"\\n---------------------------------------------------------------\")\n",
    "bagOfWords_features = [(get_bigram_bag_words(email, email_bigrams_pmiList), label) for (email, label) in remove_stopwords_emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecb74e",
   "metadata": {},
   "source": [
    "##### Combining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "422dbfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9593810444874274\n",
      "Accuracy of Fold 1: 0.9458413926499033\n",
      "Accuracy of Fold 2: 0.9535783365570599\n",
      "Accuracy of Fold 3: 0.9671179883945842\n",
      "Accuracy of Fold 4: 0.9381044487427466\n",
      "Accuracy of Fold 5: 0.9458413926499033\n",
      "Accuracy of Fold 6: 0.9497098646034816\n",
      "Accuracy of Fold 7: 0.9613152804642167\n",
      "Accuracy of Fold 8: 0.9516441005802708\n",
      "Accuracy of Fold 9: 0.9361702127659575\n",
      "Average accuracy 0.9508704061895553\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.955598     0.974715     0.965062\n",
      "spam \t 0.939293     0.896244     0.917264\n"
     ]
    }
   ],
   "source": [
    "wordFeatures = get_word_features(emailDoc, 5000)\n",
    "baseline5k_freq = [(get_bag_words(email, wordFeatures, False), label) for (email, label) in emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, baseline5k_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04486eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9361702127659575\n",
      "Accuracy of Fold 1: 0.9439071566731141\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9555125725338491\n",
      "Accuracy of Fold 4: 0.9439071566731141\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9323017408123792\n",
      "Accuracy of Fold 8: 0.9245647969052224\n",
      "Accuracy of Fold 9: 0.9323017408123792\n",
      "Average accuracy 0.9367504835589943\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.913648     0.997027     0.953518\n",
      "spam \t 0.993329     0.824474     0.901059\n"
     ]
    }
   ],
   "source": [
    "emailTokens = [token for email in emailDoc for token in email[0]]\n",
    "emailBigram_finder = BigramCollocationFinder.from_words(emailTokens)\n",
    "email_bigrams_freq = emailBigram_finder.score_ngrams(bigramMeasures.raw_freq)\n",
    "email_bigrams_freqList = [\" \".join(bigram[0]) for bigram in email_bigrams_freq[:10000]]\n",
    "bigram10k_bool = [(get_bigram_bag_words(email, email_bigrams_freqList), label) for (email, label) in emailDoc]\n",
    "cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a72e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9671179883945842\n",
      "Accuracy of Fold 1: 0.9748549323017408\n",
      "Accuracy of Fold 2: 0.9632495164410058\n",
      "Accuracy of Fold 3: 0.97678916827853\n",
      "Accuracy of Fold 4: 0.9806576402321083\n",
      "Accuracy of Fold 5: 0.97678916827853\n",
      "Accuracy of Fold 6: 0.9690522243713733\n",
      "Accuracy of Fold 7: 0.9535783365570599\n",
      "Accuracy of Fold 8: 0.9516441005802708\n",
      "Accuracy of Fold 9: 0.9574468085106383\n",
      "Average accuracy 0.9671179883945842\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.953963     0.999715     0.976303\n",
      "spam \t 0.999333     0.898620     0.946304\n"
     ]
    }
   ],
   "source": [
    "baseline5k_bigram10k_joined = []\n",
    "for i in range(len(baseline5k_freq)):\n",
    "    joinedFeatures = {feature: value for feature, value in baseline5k_freq[i][0].items()}\n",
    "    for feature, value in bigram10k_bool[i][0].items(): joinedFeatures[feature] = value\n",
    "    baseline5k_bigram10k_joined.append((joinedFeatures, baseline5k_freq[i][1]))\n",
    "cross_validation_accuracy_evaluation_metrics(10, baseline5k_bigram10k_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041ec3e",
   "metadata": {},
   "source": [
    "##### Additional experiments - changing the size of common words within Custom Tokenizer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "042f20a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 10000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9342359767891683\n",
      "Accuracy of Fold 1: 0.941972920696325\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9593810444874274\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9303675048355899\n",
      "Accuracy of Fold 8: 0.9226305609284333\n",
      "Accuracy of Fold 9: 0.9342359767891683\n",
      "Average accuracy 0.936750483558994\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.912558     0.998212     0.953465\n",
      "spam \t 0.995997     0.823043     0.901298\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 10000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9342359767891683\n",
      "Accuracy of Fold 1: 0.941972920696325\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9593810444874274\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9303675048355899\n",
      "Accuracy of Fold 8: 0.9226305609284333\n",
      "Accuracy of Fold 9: 0.9342359767891683\n",
      "Average accuracy 0.936750483558994\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.912558     0.998212     0.953465\n",
      "spam \t 0.995997     0.823043     0.901298\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 10000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9342359767891683\n",
      "Accuracy of Fold 1: 0.941972920696325\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9593810444874274\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9303675048355899\n",
      "Accuracy of Fold 8: 0.9226305609284333\n",
      "Accuracy of Fold 9: 0.9342359767891683\n",
      "Average accuracy 0.936750483558994\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.912558     0.998212     0.953465\n",
      "spam \t 0.995997     0.823043     0.901298\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 10000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9342359767891683\n",
      "Accuracy of Fold 1: 0.941972920696325\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9593810444874274\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9303675048355899\n",
      "Accuracy of Fold 8: 0.9226305609284333\n",
      "Accuracy of Fold 9: 0.9342359767891683\n",
      "Average accuracy 0.936750483558994\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.912558     0.998212     0.953465\n",
      "spam \t 0.995997     0.823043     0.901298\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 10000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9342359767891683\n",
      "Accuracy of Fold 1: 0.941972920696325\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9593810444874274\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9303675048355899\n",
      "Accuracy of Fold 8: 0.9226305609284333\n",
      "Accuracy of Fold 9: 0.9342359767891683\n",
      "Average accuracy 0.936750483558994\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.912558     0.998212     0.953465\n",
      "spam \t 0.995997     0.823043     0.901298\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 10000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9342359767891683\n",
      "Accuracy of Fold 1: 0.941972920696325\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9593810444874274\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9303675048355899\n",
      "Accuracy of Fold 8: 0.9226305609284333\n",
      "Accuracy of Fold 9: 0.9342359767891683\n",
      "Average accuracy 0.936750483558994\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.912558     0.998212     0.953465\n",
      "spam \t 0.995997     0.823043     0.901298\n",
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "\tResults for varying size of vocabulary 10000\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Each fold size: 517\n",
      "Accuracy of Fold 0: 0.9342359767891683\n",
      "Accuracy of Fold 1: 0.941972920696325\n",
      "Accuracy of Fold 2: 0.9226305609284333\n",
      "Accuracy of Fold 3: 0.9593810444874274\n",
      "Accuracy of Fold 4: 0.9458413926499033\n",
      "Accuracy of Fold 5: 0.941972920696325\n",
      "Accuracy of Fold 6: 0.9342359767891683\n",
      "Accuracy of Fold 7: 0.9303675048355899\n",
      "Accuracy of Fold 8: 0.9226305609284333\n",
      "Accuracy of Fold 9: 0.9342359767891683\n",
      "Average accuracy 0.936750483558994\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t 0.912558     0.998212     0.953465\n",
      "spam \t 0.995997     0.823043     0.901298\n"
     ]
    }
   ],
   "source": [
    "for n in [500, 1000, 2500, 3000, 5500, 7000, 10000]:\n",
    "    print(\"\\n---------------------------------------------------------------\")\n",
    "    print(\"\\n\\tResults for varying size of vocabulary {}\".format(size))\n",
    "    print(\"\\n---------------------------------------------------------------\")    \n",
    "    custom_tokenizer_emailDoc = generate_custom_tokens(emailDoc)\n",
    "    wordFeatures = get_word_features(custom_tokenizer_emailDoc, 1000)\n",
    "    bagOfWords_features = [(get_bag_words(email, wordFeatures), label) for (email, label) in custom_tokenizer_emailDoc]\n",
    "    cross_validation_accuracy_evaluation_metrics(10, bagOfWords_features)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb610fba",
   "metadata": {},
   "source": [
    "##### Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea4b6e",
   "metadata": {},
   "source": [
    "##### Baseline model with 5k features, frequency counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb0294c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.946\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "0\t0.937\t\t0.986\t\t0.961\n",
      "1\t0.967\t\t0.863\t\t0.912\n",
      "[[3441  231]\n",
      " [  49 1451]]\n"
     ]
    }
   ],
   "source": [
    "df_baseline5k_freq = nltk_features_to_dataframe(baseline5k_freq)\n",
    "x = df_baseline5k_freq.drop(\"label\", 1)\n",
    "y = df_baseline5k_freq[\"label\"]\n",
    "clf = MultinomialNB()\n",
    "predicted_y = cross_val_predict(clf, x, y, cv=10)\n",
    "get_evaluation_metrics_sklearn(y, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec4044",
   "metadata": {},
   "source": [
    "##### Baseline model with 10k features, frequency counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e2294335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "0\t0.96\t\t0.969\t\t0.965\n",
      "1\t0.925\t\t0.905\t\t0.915\n",
      "[[3526  146]\n",
      " [ 113 1387]]\n"
     ]
    }
   ],
   "source": [
    "df_bigram10k_bool = nltk_features_to_dataframe(bigram10k_bool)\n",
    "x = df_bigram10k_bool.drop(\"label\", 1)\n",
    "y = df_bigram10k_bool[\"label\"]\n",
    "clf = MultinomialNB()\n",
    "predicted_y = cross_val_predict(clf, x, y, cv=10)\n",
    "get_evaluation_metrics_sklearn(y, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f4e5d",
   "metadata": {},
   "source": [
    "##### Analysis using additional classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddb09bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EmailCorpus = []\n",
    "vocabSize = []\n",
    "for email in emailDoc:\n",
    "    listToStr = ' '.join([str(element) for element in email[0]])\n",
    "    element = 0 if email[1] == 'ham' else 1\n",
    "    EmailCorpus.append((listToStr, email[1]))\n",
    "text = []\n",
    "label = []\n",
    "for dt in EmailCorpus:\n",
    "    text.append(dt[0])\n",
    "    label.append(dt[1])\n",
    "totalCount = Counter()\n",
    "for i in range(len(text)):\n",
    "    for word in text[i].split(\" \"):\n",
    "        totalCount[word] += 1\n",
    "#sorting in descending order (i.e., word with the highest frequency appears first)\n",
    "vocab = sorted(totalCount, key=totalCount.get, reverse=True)\n",
    "vocabSize = len(vocab)\n",
    "wordToidx = {}\n",
    "#print vocab_size\n",
    "for idx, word in enumerate(vocab):\n",
    "    wordToidx[word] = idx\n",
    "#converting all titles to vectors\n",
    "wordVectors = np.zeros((len(text), len(vocab)), dtype=np.int_)\n",
    "for idx,data in enumerate(text):\n",
    "    wordVectors[idx] = text_to_vector(data)\n",
    "wordVectors.shape\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(text)\n",
    "vectors.shape\n",
    "features = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b231a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, label, test_size=0.15, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad466bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing multiple classifier models\n",
    "svm = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knn = KNeighborsClassifier(n_neighbors=49)\n",
    "mnb = MultinomialNB(alpha=0.2)\n",
    "dtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=31, random_state=111)\n",
    "clfs = {'Support Vector' : svm,'K-Nearest Neighbors' : knn, 'Naive Bayes': mnb, 'Decision Tree': dtc, 'Logistic Regression': lrc, 'Random Forest': rfc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f36bd71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Support Vector', [0.9871134020618557]),\n",
       " ('K-Neareast Neighbors', [0.9432989690721649]),\n",
       " ('Naive Bayes', [0.9677835051546392]),\n",
       " ('Decision Tree', [0.9484536082474226]),\n",
       " ('Logistic Regression', [0.9548969072164949]),\n",
       " ('Random Forest', [0.9690721649484536]),\n",
       " ('Support Vector', [0.9871134020618557]),\n",
       " ('K-Nearest Neighbors', [0.9432989690721649]),\n",
       " ('Naive Bayes', [0.9677835051546392]),\n",
       " ('Decision Tree', [0.9484536082474226]),\n",
       " ('Logistic Regression', [0.9548969072164949]),\n",
       " ('Random Forest', [0.9690721649484536])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key,value in clfs.items():\n",
    "    train(value, x_train, y_train)\n",
    "    prediction = predict(value, x_test)\n",
    "    predScores_wordVectors.append((key, [accuracy_score(y_test , prediction)]))\n",
    "predScores_wordVectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
